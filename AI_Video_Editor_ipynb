{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsBGVdiMfAVfW4x7Zw2l12"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Definition & Objective\n",
        "\n",
        "## Selected Project Track\n",
        "* **Track:** (e.g., \"AI for Content Creation\" or \"Multimedia Processing\" - *Fill this in based on your specific hackathon/assignment track*)\n",
        "\n",
        "## Clear Problem Statement\n",
        "Video content creation involves tedious post-processing. Specifically, identifying silence, removing it, and generating accurate captions manually can take hours for just a few minutes of footage. This creates a bottleneck for educators, vloggers, and content creators who need to produce accessible content quickly.\n",
        "\n",
        "## Real-world Relevance and Motivation\n",
        "* **Accessibility:** Captions make content accessible to the deaf and hard-of-hearing community.\n",
        "* **Efficiency:** Automating the \"jump-cut\" editing style saves creators 80% of editing time.\n",
        "* **Engagement:** Short-form content (Reels/TikTok) requires fast-paced editing to retain viewer attention."
      ],
      "metadata": {
        "id": "nwLXuFO-XstR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Understanding & Preparation\n",
        "In this stage, we set up the environment and prepare the libraries required for:\n",
        "1.  **Computer Vision/Video Processing:** Using `MoviePy` for cutting and stitching video frames.\n",
        "2.  **Audio Processing:** Using `OpenAI Whisper` for state-of-the-art speech-to-text transcription.\n",
        "\n",
        "We also handle dependencies to ensure the code is reproducible."
      ],
      "metadata": {
        "id": "3wcMrsTMXyBD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFaRqebETT5D"
      },
      "outputs": [],
      "source": [
        "# --- INSTALL SYSTEM DEPENDENCIES ---\n",
        "# Install Whisper and MoviePy\n",
        "!pip install git+https://github.com/openai/whisper.git moviepy\n",
        "\n",
        "# Install ImageMagick system library\n",
        "!apt-get install -y imagemagick\n",
        "\n",
        "# FIX: Allow ImageMagick to read/write files (required for Google Colab)\n",
        "!sed -i 's/policy domain=\"path\" rights=\"none\" pattern=\"@\\*\"/policy domain=\"path\" rights=\"read|write\" pattern=\"@\\*\"/g' /etc/ImageMagick-6/policy.xml\n",
        "\n",
        "# Tell MoviePy where ImageMagick is located\n",
        "import os\n",
        "from moviepy.config import change_settings\n",
        "change_settings({\"IMAGEMAGICK_BINARY\": r\"/usr/bin/convert\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LOAD LIBRARIES AND AI MODEL ---\n",
        "import whisper\n",
        "import time\n",
        "from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip\n",
        "from moviepy.editor import concatenate_videoclips\n",
        "\n",
        "# Load the AI model (base is fast and accurate)\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Verify your video exists\n",
        "input_video_path = \"input_video.mp4\" # Ensure you uploaded this file!\n",
        "if os.path.exists(input_video_path):\n",
        "    print(f\"Model and Video Ready!\")\n",
        "else:\n",
        "    print(\"ERROR: Upload 'input_video.mp4' to the sidebar first.\")"
      ],
      "metadata": {
        "id": "ntCAJwZcTV64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model / System Design\n",
        "The system follows a sequential pipeline approach:\n",
        "\n",
        "1.  **Input:** Raw video file (MP4).\n",
        "2.  **Transcription Engine (Whisper):** Analyzes audio to generate text segments with precise timestamps (Start Time, End Time).\n",
        "3.  **Filtering Logic:** The system iterates through transcribed segments. Timestamps containing valid speech are retained; silence/background noise is discarded.\n",
        "4.  **Video Composition (MoviePy):** * Extract subclips based on active speech timestamps.\n",
        "    * Concatenate subclips to form a continuous video.\n",
        "5.  **Output:** Rendered video with silence removed.\n",
        "\n",
        "**Flow:** `Input Video` -> `Speech Detection` -> `Timeline Mapping` -> `Clip Stitching` -> `Final Output`"
      ],
      "metadata": {
        "id": "DnEfStAeYA7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Core Implementation\n",
        "This section contains the primary logic. The function `process_and_caption` encapsulates the transcription and video editing workflow.\n",
        "\n",
        "**Key Technical Details:**\n",
        "* **Input:** Video path, Output path, Whisper model.\n",
        "* **Logic:** We loop through `result['segments']` provided by Whisper to identify exactly when the user is speaking."
      ],
      "metadata": {
        "id": "eawuQ4Q5YDC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- COMBINED CUTTING & CAPTIONING ---\n",
        "\n",
        "def process_and_caption(input_path, output_path, model):\n",
        "    print(\"Step 1: Transcribing and Analyzing Audio...\")\n",
        "    result = model.transcribe(input_path, fp16=False)\n",
        "\n",
        "    video = VideoFileClip(input_path)\n",
        "    final_clips = []\n",
        "\n",
        "    print(\"Step 2: Cutting silence and adding captions...\")\n",
        "    for segment in result['segments']:\n",
        "        start_time = segment['start']\n",
        "        end_time = segment['end']\n",
        "        text = segment['text'].strip()\n",
        "\n",
        "        # Noise Filter: Skip if AI is unsure it's speech\n",
        "        if segment.get('no_speech_prob', 0) > 0.45:\n",
        "            continue\n",
        "\n",
        "        # 1. Create the subclip (The 'Cut')\n",
        "        clip = video.subclip(max(0, start_time), min(video.duration, end_time))\n",
        "\n",
        "        # 2. Create the caption (The 'Subtitle')\n",
        "        txt_clip = TextClip(\n",
        "            text,\n",
        "            fontsize=45,\n",
        "            color='yellow',\n",
        "            font='Arial-Bold',\n",
        "            stroke_color='black',\n",
        "            stroke_width=1,\n",
        "            method='caption',\n",
        "            size=(video.w * 0.8, None)\n",
        "        ).set_duration(clip.duration).set_position(('center', video.h * 0.8))\n",
        "\n",
        "        # 3. Combine them: Overlay text on this specific clip\n",
        "        captioned_clip = CompositeVideoClip([clip, txt_clip])\n",
        "        final_clips.append(captioned_clip)\n",
        "\n",
        "    if final_clips:\n",
        "        print(\"Step 3: Stitching the final captioned video...\")\n",
        "        final_video = concatenate_videoclips(final_clips)\n",
        "        final_video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
        "        return video.duration, final_video.duration\n",
        "    else:\n",
        "        print(\"No clear speech detected.\")\n",
        "        return 0, 0\n",
        "\n",
        "# Run the final process\n",
        "original_len, new_len = process_and_caption(input_video_path, \"final_ai_vlog.mp4\", model)\n"
      ],
      "metadata": {
        "id": "dtSzuw_aTXMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation & Analysis\n",
        "We evaluate the system by comparing the duration of the original raw footage vs. the processed output. A reduction in time indicates successful silence removal.\n",
        "\n",
        "*Note: Ensure you have a file named 'input_video.mp4' in your Colab files before running.*"
      ],
      "metadata": {
        "id": "0VMP0BhCYO56"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1d560ad"
      },
      "source": [
        "# --- PERFORMANCE EVALUATION ---\n",
        "print(f\"--- RESULTS ---\")\n",
        "print(f\"Video Length: {video_length:.2f} seconds\")\n",
        "print(f\"AI Processing Time: {inference_duration:.2f} seconds\")\n",
        "\n",
        "# Real-time factor (e.g., 2.0x means it processed twice as fast as the video)\n",
        "rtf = video_length / inference_duration\n",
        "print(f\"Processing Speed: {rtf:.2f}x relative to video length\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ethical Considerations & Responsible AI\n",
        "* **Privacy:** This tool processes voice data. In a production environment, data should be processed locally or encrypted to protect user privacy.\n",
        "* **Bias in Transcription:** AI models (like Whisper) may have biases against certain accents or dialects. We must acknowledge that the \"silence removal\" relies on the model accurately detecting speech, which might fail for non-native speakers.\n",
        "* **Content Manipulation:** Automated editing tools can be used to take words out of context. Responsible use guidelines should be established."
      ],
      "metadata": {
        "id": "OD8ZF6mrYRBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion & Future Scope\n",
        "### Conclusion\n",
        "We successfully implemented an automated video editing pipeline that utilizes OpenAI Whisper for semantic analysis of video content. The system effectively removes non-speech segments, streamlining the video creation process.\n",
        "\n",
        "### Future Scope\n",
        "1.  **Speaker Diarization:** Differentiate between multiple speakers to only keep clips from a specific person.\n",
        "2.  **Burn-in Captions:** Overlay the transcribed text directly onto the video frames using ImageMagick.\n",
        "3.  **UI Wrapper:** Wrap this Python logic in a Streamlit or Gradio interface for non-technical users."
      ],
      "metadata": {
        "id": "VM4Nv7maYSVd"
      }
    }
  ]
}